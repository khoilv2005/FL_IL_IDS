"""
Incremental Data Loader - Load data for Federated Class Incremental Learning.

Adapts to the new data structure generated by step2_federated_splits.py:
- Reads metadata.json for task structure
- Loads individual client data files (client_{id}_train.npz)
- Filters data dynamically based on task classes
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Set

import numpy as np
import torch

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IncrementalDataLoader:
    """
    Loads data for incremental learning from pre-split federated data.
    
    Expects data directory structure:
    - data_dir/
        - metadata.json
        - client_0_train.npz
        - ...
        - global_test_data.npz
    """
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.metadata_file = self.data_dir / "metadata.json"
        self.test_file = self.data_dir / "global_test_data.npz"
        
        if not self.data_dir.exists():
            raise FileNotFoundError(f"Data directory {data_dir} not found")
            
        self.metadata = self._load_metadata()
        self.task_classes = self._parse_task_classes()
        
        # Cache for test data (loaded lazily once)
        self._test_data = None
        
        # Input shape (lazy loaded from first client file)
        self._input_shape = None
        
        logger.info(f"Initialized IncrementalLoader from {data_dir}")
        logger.info(f"Task structure: {len(self.task_classes)} tasks found")
    
    @property
    def input_shape(self):
        """Get input shape from first available client file."""
        if self._input_shape is None:
            # Find a client file
            for client_file in self.data_dir.glob("client_*_train.npz"):
                try:
                    data = np.load(client_file)
                    X = data['X_train']
                    # Shape is (N, seq_len, features) or (N, seq_len)
                    if X.ndim == 2:
                        self._input_shape = (X.shape[1],)
                    else:
                        self._input_shape = X.shape[1:]  # (seq_len, features)
                    break
                except Exception as e:
                    logger.warning(f"Could not read {client_file}: {e}")
                    continue
            
            if self._input_shape is None:
                # Fallback: try test file
                try:
                    data = np.load(self.test_file)
                    X = data['X_test']
                    if X.ndim == 2:
                        self._input_shape = (X.shape[1],)
                    else:
                        self._input_shape = X.shape[1:]
                except Exception:
                    self._input_shape = (46, 1)  # Default fallback
                    
        return self._input_shape
        
    def _load_metadata(self) -> Dict:
        if not self.metadata_file.exists():
            raise FileNotFoundError(f"Metadata file not found at {self.metadata_file}")
        
        with open(self.metadata_file, "r") as f:
            return json.load(f)
            
    def _parse_task_classes(self) -> Dict[int, List[int]]:
        """Parse task classes from metadata."""
        if "task_structure" not in self.metadata:
            # Fallback for old format or error
            logger.warning("Metadata missing 'task_structure', assuming old format or error.")
            return {}
            
        raw_map = self.metadata["task_structure"]["task_classes"]
        # Convert string keys to int
        return {int(k): v for k, v in raw_map.items()}
    
    def get_task_classes(self, task_id: int) -> List[int]:
        return self.task_classes.get(task_id, [])
    
    def get_total_classes(self) -> int:
        if "task_structure" in self.metadata:
            return self.metadata["task_structure"]["total_classes"]
        return 0
        
    def get_num_tasks(self) -> int:
        return len(self.task_classes)
    
    @property
    def num_tasks(self) -> int:
        """Alias for get_num_tasks() for backward compatibility."""
        return self.get_num_tasks()

    def get_client_data(self, client_id: Union[int, str], task_id: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get training data for a specific client and task.
        Filters the client's local data to include only classes for this task.
        
        Args:
            client_id: ID of the client
            task_id: Task ID to load data for
            
        Returns:
            (X, y) tensors. Empty if client has no data for this task.
        """
        # Handle client_id being int or str
        client_file = self.data_dir / f"client_{client_id}_train.npz"
        if not client_file.exists():
            # Try exploring directory if naming is slightly different or check metadata
            # For now assume standard naming
            logger.debug(f"Client file {client_file} not found")
            return torch.empty(0), torch.empty(0)
            
        # Load client data
        try:
            data = np.load(client_file)
            X_train = data['X_train']
            y_train = data['y_train']
        except Exception as e:
            logger.error(f"Error loading {client_file}: {e}")
            return torch.empty(0), torch.empty(0)
            
        # Filter for current task classes
        target_classes = set(self.get_task_classes(task_id))
        if not target_classes:
            logger.warning(f"No classes found for task {task_id}")
            return torch.empty(0), torch.empty(0)
            
        # Mask: include only samples belonging to current task classes
        mask = np.isin(y_train, list(target_classes))
        
        if not np.any(mask):
            return torch.empty(0), torch.empty(0)

        X_task = X_train[mask]
        y_task = y_train[mask]
        
        # Convert to tensor
        # Ensure float32 for features (X) and long for labels (y)
        return torch.from_numpy(X_task).float(), torch.from_numpy(y_task).long()
        
    def get_test_data(self, task_id: int, cumulative: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get global test data.
        
        Args:
            task_id: Current task ID
            cumulative: If True, return test data for all classes seen so far (0..task_id).
                       If False, return test data only for current task classes.
        """
        if self._test_data is None:
            if not self.test_file.exists():
                logger.error(f"Test file {self.test_file} not found")
                return torch.empty(0), torch.empty(0)
            try:
                data = np.load(self.test_file)
                self._test_data = (data['X_test'], data['y_test'])
            except Exception as e:
                logger.error(f"Error loading test data: {e}")
                return torch.empty(0), torch.empty(0)
            
        X_test_all, y_test_all = self._test_data
        
        # Determine target classes
        target_classes = []
        if cumulative:
            for t in range(task_id + 1):
                target_classes.extend(self.get_task_classes(t))
        else:
            target_classes = self.get_task_classes(task_id)
            
        if not target_classes:
             return torch.empty(0), torch.empty(0)

        mask = np.isin(y_test_all, target_classes)
        X_task = X_test_all[mask]
        y_task = y_test_all[mask]
        
        return torch.from_numpy(X_task).float(), torch.from_numpy(y_task).long()

    def get_all_client_ids(self) -> List[int]:
        """Get list of all client IDs from metadata."""
        if "client_allocation" in self.metadata and "client_join_task" in self.metadata["client_allocation"]:
             # Keys are string integers
             return [int(k) for k in self.metadata["client_allocation"]["client_join_task"].keys()]
        return []
