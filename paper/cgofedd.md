Ráº¥t tá»‘t ğŸ‘
DÆ°á»›i Ä‘Ã¢y lÃ  **mÃ£ giáº£ Algorithm 1 Ä‘Æ°á»£c viáº¿t láº¡i hoÃ n toÃ n báº±ng tiáº¿ng Viá»‡t**, **khÃ´ng dÃ¹ng kÃ½ hiá»‡u (Eq.x)** ná»¯a mÃ  **viáº¿t tháº³ng cÃ´ng thá»©c**, kÃ¨m **chÃº thÃ­ch ngáº¯n gá»n ngay táº¡i bÆ°á»›c Ä‘Ã³** Ä‘á»ƒ báº¡n Ä‘á»c *khÃ´ng cáº§n láº­t láº¡i bÃ i bÃ¡o*.

---

# THUáº¬T TOÃN: Chiáº¿n lÆ°á»£c Tá»‘i Æ°u Gradient CÃ³ RÃ ng buá»™c cho Federated Class Incremental Learning (CGoFed)

---

## **Äáº¦U VÃ€O**

* (K): sá»‘ lÆ°á»£ng client
* (T): sá»‘ lÆ°á»£ng task

## **Äáº¦U RA**

* (\Theta^{T,g}): mÃ´ hÃ¬nh toÃ n cá»¥c (hoáº·c cÃ¡ nhÃ¢n hoÃ¡) sau khi há»c xong task cuá»‘i

---

## **KHá»I Táº O**

1. Vá»›i má»—i client (k):

   * Chuáº©n bá»‹ chuá»—i dá»¯ liá»‡u theo task:
     $$
     {D_k^1, D_k^2, \dots, D_k^T}
     $$
2. Khá»Ÿi táº¡o tham sá»‘ mÃ´ hÃ¬nh ban Ä‘áº§u cho má»—i client:
   $$
   \Theta_k^{init}
   $$

---

## **VÃ’NG Láº¶P THEO TASK**

3. **Cho** (t = 1) **Ä‘áº¿n** (T) **lÃ m**:

---

## ğŸ”¹ PHáº¦N A â€” THá»°C HIá»†N TRÃŠN CLIENT (CHáº Y SONG SONG)

4. **Cho má»—i client** (k = 1 \dots K) **cháº¡y song song**:

---

### **TRÆ¯á»œNG Há»¢P 1: TASK Äáº¦U TIÃŠN ((t = 1))**

5. **Huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a client k báº±ng loss phÃ¢n loáº¡i (Cross-Entropy):**
   $$
   L_k(\Theta_k^1)
   =
   \frac{1}{n_k^1}
   \sum_{i=1}^{n_k^1}
   \ell\big(f(x_{k,i}^1;\Theta_k^1),; y_{k,i}^1\big)
   $$

6. **TÃ­nh gradient cá»§a loss:**
   $$
   g = \nabla_{\Theta_k^1} L_k(\Theta_k^1)
   $$

7. **Cáº­p nháº­t gradient theo khÃ´ng gian trá»±c giao (náº¿u cÃ³ memory):**
   $$
   g \leftarrow g - g M^{0}(M^{0})^\top
   $$
   *(vá»›i task Ä‘áº§u, (M^{0}) gáº§n nhÆ° rá»—ng)*

8. **Cáº­p nháº­t tham sá»‘ mÃ´ hÃ¬nh:**
   $$
   \Theta_k^1 \leftarrow \Theta_k^1 - \eta g
   $$

---

### **TRÆ¯á»œNG Há»¢P 2: TASK THá»¨ (t > 1)**

9. **Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i hÃ m loss tá»•ng (loss phÃ¢n loáº¡i + regularization):**
   $$
   \min_{\Theta_k^t}
   \Bigg(
   \frac{1}{n_k^t}
   \sum_{i=1}^{n_k^t}
   \ell\big(f(x_{k,i}^t;\Theta_k^t),; y_{k,i}^t\big)
   ;+;
   A(\Theta_k^t,\Theta^{old})
   \Bigg)
   $$

Trong Ä‘Ã³ regularization:
$$
A(\Theta_k^t,\Theta^{old})
==========================
\sum_{j < t}
\sum_{i \in \pi}
w_i^j
\left|
\Theta_k^t - \Theta_i^j
\right|_2^2
$$
---

10. **TÃ­nh há»‡ sá»‘ siáº¿t rÃ ng buá»™c (\mu_t):**

* HÃ m decay:
  $$
  f(\alpha,t) = \alpha^t
  $$

* CÃ´ng thá»©c xÃ¡c Ä‘á»‹nh:
  $$
  \mu_t =
  \begin{cases}
  \mu_{init},\alpha^t, & \text{náº¿u } AF < \tau \
  \mu_{init},\alpha^{t - t_\tau}, & \text{náº¿u } AF \ge \tau
  \end{cases}
  $$

---

11. **TÃ­nh gradient cá»§a loss tá»•ng:**
    $$
    g = \nabla_{\Theta_k^t} L_k(\Theta_k^t)
    $$

---

12. **Chá»‰nh gradient Ä‘á»ƒ trÃ¡nh phÃ¡ task cÅ© (gradient constraint):**
    $$
    g \leftarrow g - \mu_t , g M^{t-1}(M^{t-1})^\top
    $$

> ğŸ‘‰ BÆ°á»›c nÃ y Ä‘áº£m báº£o:
>
> * gradient váº«n giáº£m loss task má»›i
> * nhÆ°ng bá»‹ â€œbáº» hÆ°á»›ngâ€ Ä‘á»ƒ Ã­t lÃ m há»ng task cÅ©

---

13. **Cáº­p nháº­t tham sá»‘ mÃ´ hÃ¬nh:**
    $$
    \Theta_k^t \leftarrow \Theta_k^t - \eta g
    $$

---

## ğŸ”¹ SAU KHI TRAIN XONG TASK (t) (CLIENT LÆ¯U KÃ á»¨C)

14. **Táº¡o ma tráº­n biá»ƒu diá»…n (representation) cho task t:**
    $$
    R_k^t = F(\Theta_k^t, X^t)
    $$

---

15. **PhÃ¢n rÃ£ SVD Ä‘á»ƒ tÃ¬m cÃ¡c hÆ°á»›ng quan trá»ng cá»§a task:**
    $$
    R_k^t = U_k^t \Sigma_k^t (V_k^t)^\top
    $$

---

16. **TÃ­nh trá»ng sá»‘ cho má»—i vector cÆ¡ sá»Ÿ (Ä‘á»™ quan trá»ng):**
    $$
    \Lambda_k^t = \frac{1}{1 + e^{-\Sigma_k^t}}
    $$

---

17. **Táº¡o vÃ  lÆ°u ma tráº­n nhá»› (memory subspace):**
    $$
    M_k^t =
    \big[
    \lambda_1^t u_1^t,;
    \lambda_2^t u_2^t,;
    \dots
    \big]
    $$

---

18. **Gá»­i lÃªn server:**

* Tham sá»‘ mÃ´ hÃ¬nh (\Theta_k^t)
* Ma tráº­n biá»ƒu diá»…n (R_k^t)

---

## ğŸ”¹ PHáº¦N B â€” THá»°C HIá»†N TRÃŠN SERVER

19. **Náº¿u lÃ  task Ä‘áº§u tiÃªn ((t = 1)) thÃ¬ gá»™p mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n:**
    $$
    \Theta_k^{1,g} = \Theta_k^1
    $$

---

20. **TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c task cá»§a cÃ¡c client:**
    $$
    \phi_i^t
    =
    \left|
    R_k^t - R_i^{t-1}
    \right|_2
    $$

---

21. **Gá»™p mÃ´ hÃ¬nh theo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (personalized aggregation):**
    $$
    \Theta_k^{t,g}
    =
    \Theta_k^t

*

\sum_{i \neq k}
\frac{\phi_i^t}{\sum_{j \neq k} \phi_j^t}
\Theta_i^t
$$

---

22. **Káº¿t thÃºc task t**

---

## âœ… **TÃ“M Táº®T Báº°NG 1 CÃ‚U**

> **CGoFed há»c báº±ng cÃ¡ch:**
> dÃ¹ng **loss Ä‘á»ƒ há»c task má»›i**,
> **báº» hÆ°á»›ng gradient Ä‘á»ƒ khÃ´ng phÃ¡ task cÅ©**,
> vÃ  **thÃªm regularization + aggregation thÃ´ng minh trÃªn server** Ä‘á»ƒ cÃ¡c client há»— trá»£ nhau Ä‘Ãºng task.

---

Náº¿u báº¡n muá»‘n, bÆ°á»›c tiáº¿p theo mÃ¬nh cÃ³ thá»ƒ:

* viáº¿t **phiÃªn báº£n â€œgiáº£i thÃ­ch báº±ng lá»i ngay dÆ°á»›i tá»«ng dÃ²ng mÃ£ giáº£â€**, hoáº·c
* lÃ m **vÃ­ dá»¥ sá»‘ 2D** Ä‘á»ƒ báº¡n tháº¥y gradient bá»‹ chiáº¿u nhÆ° tháº¿ nÃ o.
