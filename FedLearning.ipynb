{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def make_auto_filename(prefix: str, suffix: str = \"\", ext: str = \"pth\"):\n",
    "    \"\"\"T\u1ea1o t\u00ean file t\u1ef1 \u0111\u1ed9ng: <prefix>_<YYYYmmdd_HHMMSS>[_suffix].ext\n",
    "\n",
    "    V\u00ed d\u1ee5:\n",
    "        fedplus_global_model_20251126_101530.pth\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    parts = [prefix, ts]\n",
    "    if suffix:\n",
    "        parts.append(suffix)\n",
    "    return \"_\".join(parts) + f\".{ext}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "try:\n",
    "    from torch.amp import autocast as torch_autocast, GradScaler\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast as torch_autocast, GradScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "torch.set_printoptions(linewidth=120, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Path to step2-version3 output\n",
    "    \"data_dir\": \"/kaggle/input/data-10clients\",\n",
    "    \"output_dir\": \"./results\",\n",
    "    \"checkpoint_dir\": \"./checkpoints\",  # NEW: Checkpoint directory\n",
    "\n",
    "    # Federation\n",
    "    \"num_clients\": 10,\n",
    "    \"algorithm\": \"fedavgm\",   # \"fedavg\" or \"fedprox\" or \"fedavgm\" or \"fedplus\"\n",
    "\n",
    "    # Fedavgm Parameters \n",
    "    \"server_momentum\": 0.9,\n",
    "    \"server_lr\": 1,\n",
    "\n",
    "    # Model\n",
    "    \"input_shape\": None,\n",
    "    \"num_classes\": None,\n",
    "\n",
    "    # Training\n",
    "    \"num_rounds\": 5,\n",
    "    \"local_epochs\": 3,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 1024,\n",
    "    \"mu\": 0.01,\n",
    "\n",
    "    # Device\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "\n",
    "    # Eval & Checkpoint\n",
    "    \"eval_every\": 1,\n",
    "    \"save_checkpoint_every\": 1,  # NEW: Save checkpoint every N rounds\n",
    "    \"resume_from_checkpoint\": None,  # NEW: Path to checkpoint to resume from\n",
    "}\n",
    "\n",
    "device = CONFIG[\"device\"]\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Algorithm: {CONFIG['algorithm']}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Manager & Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    Qu\u1ea3n l\u00fd checkpoint cho Federated Learning.\n",
    "    L\u01b0u v\u00e0 load model, optimizer state, history sau m\u1ed7i round.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_dir: str, algorithm: str, num_clients: int):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.algorithm = algorithm\n",
    "        self.num_clients = num_clients\n",
    "        self.run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # \u0110\u1ed5i t\u00ean folder th\u00e0nh algorithm_num_clients (v\u00ed d\u1ee5: fedplus_5)\n",
    "        self.folder_name = f\"{algorithm}_{num_clients}\"\n",
    "        self.run_dir = os.path.join(checkpoint_dir, self.folder_name)\n",
    "        os.makedirs(self.run_dir, exist_ok=True)\n",
    "        print(f\"\u2713 Checkpoint directory: {self.run_dir}\")\n",
    "    \n",
    "    def get_run_id(self):\n",
    "        \"\"\"Tr\u1ea3 v\u1ec1 run_id \u0111\u1ec3 d\u00f9ng chung v\u1edbi results.\"\"\"\n",
    "        return self.run_id\n",
    "    \n",
    "    def get_folder_name(self):\n",
    "        \"\"\"Tr\u1ea3 v\u1ec1 folder name \u0111\u1ec3 d\u00f9ng chung v\u1edbi results.\"\"\"\n",
    "        return self.folder_name\n",
    "    \n",
    "    def save_checkpoint(self, round_idx: int, server, history: Dict, \n",
    "                        config: Dict, extra_info: Dict = None):\n",
    "        \"\"\"\n",
    "        L\u01b0u checkpoint sau m\u1ed7i round.\n",
    "        \n",
    "        Args:\n",
    "            round_idx: Round index (0-based)\n",
    "            server: FederatedServerV2 instance\n",
    "            history: Training history dict\n",
    "            config: CONFIG dict\n",
    "            extra_info: Additional info to save\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            \"round\": round_idx,\n",
    "            \"global_model_state\": server.global_model.state_dict(),\n",
    "            \"velocity\": server.velocity if hasattr(server, 'velocity') else None,\n",
    "            \"history\": history,\n",
    "            \"config\": {k: str(v) if not isinstance(v, (int, float, str, bool, type(None))) else v \n",
    "                      for k, v in config.items()},\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        if extra_info:\n",
    "            checkpoint[\"extra_info\"] = extra_info\n",
    "        \n",
    "        # Save checkpoint\n",
    "        ckpt_path = os.path.join(self.run_dir, f\"checkpoint_round_{round_idx+1:03d}.pt\")\n",
    "        torch.save(checkpoint, ckpt_path)\n",
    "        \n",
    "        # Also save as latest\n",
    "        latest_path = os.path.join(self.run_dir, \"checkpoint_latest.pt\")\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Save history as JSON for easy viewing\n",
    "        hist_path = os.path.join(self.run_dir, \"history.json\")\n",
    "        with open(hist_path, \"w\") as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        \n",
    "        print(f\"  \ud83d\udcbe Saved checkpoint: round {round_idx+1}\")\n",
    "        return ckpt_path\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_checkpoint(checkpoint_path: str, server, device: str):\n",
    "        \"\"\"\n",
    "        Load checkpoint v\u00e0 restore state.\n",
    "        \n",
    "        Returns:\n",
    "            round_idx: Round \u0111\u00e3 train xong\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        print(f\"\\n\ud83d\udcc2 Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Restore model\n",
    "        server.global_model.load_state_dict(checkpoint[\"global_model_state\"])\n",
    "        \n",
    "        # Restore velocity (for FedAvgM)\n",
    "        if checkpoint.get(\"velocity\") is not None:\n",
    "            server.velocity = checkpoint[\"velocity\"]\n",
    "        \n",
    "        # Restore history\n",
    "        server.history = checkpoint[\"history\"]\n",
    "        \n",
    "        round_idx = checkpoint[\"round\"]\n",
    "        print(f\"  \u2713 Restored from round {round_idx + 1}\")\n",
    "        print(f\"  \u2713 Last accuracy: {checkpoint['history']['test_accuracy'][-1]*100:.2f}%\")\n",
    "        \n",
    "        return round_idx, checkpoint[\"history\"]\n",
    "    \n",
    "    def get_all_checkpoints(self):\n",
    "        \"\"\"List all checkpoints in run directory.\"\"\"\n",
    "        ckpts = []\n",
    "        for f in os.listdir(self.run_dir):\n",
    "            if f.startswith(\"checkpoint_round_\") and f.endswith(\".pt\"):\n",
    "                ckpts.append(os.path.join(self.run_dir, f))\n",
    "        return sorted(ckpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"\n",
    "    T\u00ednh to\u00e1n c\u00e1c metrics cho multiclass classification:\n",
    "    - Accuracy, Precision, Recall, F1 (macro, micro, weighted)\n",
    "    - ROC-AUC (one-vs-rest)\n",
    "    - Per-class metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                           y_proba: np.ndarray = None, num_classes: int = None):\n",
    "        \"\"\"\n",
    "        T\u00ednh t\u1ea5t c\u1ea3 metrics.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Ground truth labels\n",
    "            y_pred: Predicted labels\n",
    "            y_proba: Predicted probabilities (softmax output) - for AUC\n",
    "            num_classes: Number of classes\n",
    "            \n",
    "        Returns:\n",
    "            Dict ch\u1ee9a t\u1ea5t c\u1ea3 metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Precision, Recall, F1 - multiple averaging strategies\n",
    "        for avg in [\"macro\", \"micro\", \"weighted\"]:\n",
    "            metrics[f\"precision_{avg}\"] = precision_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "            metrics[f\"recall_{avg}\"] = recall_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "            metrics[f\"f1_{avg}\"] = f1_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        precision_per_class, recall_per_class, f1_per_class, support = \\\n",
    "            precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "        \n",
    "        metrics[\"precision_per_class\"] = precision_per_class.tolist()\n",
    "        metrics[\"recall_per_class\"] = recall_per_class.tolist()\n",
    "        metrics[\"f1_per_class\"] = f1_per_class.tolist()\n",
    "        metrics[\"support_per_class\"] = support.tolist()\n",
    "        \n",
    "        # ROC-AUC (requires probability scores)\n",
    "        if y_proba is not None and num_classes is not None:\n",
    "            try:\n",
    "                # Binarize labels for multiclass AUC\n",
    "                y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "                \n",
    "                # Handle case where not all classes are present\n",
    "                if y_true_bin.shape[1] == 1:\n",
    "                    y_true_bin = np.hstack([1 - y_true_bin, y_true_bin])\n",
    "                \n",
    "                # Macro AUC (one-vs-rest)\n",
    "                auc_ovr = roc_auc_score(y_true_bin, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "                metrics[\"auc_macro_ovr\"] = auc_ovr\n",
    "                \n",
    "                # Weighted AUC\n",
    "                auc_weighted = roc_auc_score(y_true_bin, y_proba, average=\"weighted\", multi_class=\"ovr\")\n",
    "                metrics[\"auc_weighted_ovr\"] = auc_weighted\n",
    "                \n",
    "                # Per-class AUC\n",
    "                auc_per_class = []\n",
    "                for i in range(num_classes):\n",
    "                    if y_true_bin[:, i].sum() > 0:  # Only if class exists in y_true\n",
    "                        try:\n",
    "                            auc_i = roc_auc_score(y_true_bin[:, i], y_proba[:, i])\n",
    "                            auc_per_class.append(auc_i)\n",
    "                        except:\n",
    "                            auc_per_class.append(None)\n",
    "                    else:\n",
    "                        auc_per_class.append(None)\n",
    "                metrics[\"auc_per_class\"] = auc_per_class\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  \u26a0 Warning: Could not compute AUC: {e}\")\n",
    "                metrics[\"auc_macro_ovr\"] = None\n",
    "                metrics[\"auc_weighted_ovr\"] = None\n",
    "                metrics[\"auc_per_class\"] = None\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_metrics_summary(metrics: Dict, round_idx: int = None):\n",
    "        \"\"\"In t\u00f3m t\u1eaft metrics.\"\"\"\n",
    "        header = f\"Round {round_idx+1}\" if round_idx is not None else \"Evaluation\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"\ud83d\udcca METRICS SUMMARY - {header}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Accuracy:           {metrics['accuracy']*100:.2f}%\")\n",
    "        print(f\"  F1 (macro):         {metrics['f1_macro']*100:.2f}%\")\n",
    "        print(f\"  F1 (weighted):      {metrics['f1_weighted']*100:.2f}%\")\n",
    "        print(f\"  Precision (macro):  {metrics['precision_macro']*100:.2f}%\")\n",
    "        print(f\"  Recall (macro):     {metrics['recall_macro']*100:.2f}%\")\n",
    "        if metrics.get(\"auc_macro_ovr\") is not None:\n",
    "            print(f\"  AUC (macro OvR):    {metrics['auc_macro_ovr']*100:.2f}%\")\n",
    "        print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonIIDAnalyzer:\n",
    "    \"\"\"\n",
    "    Ph\u00e2n t\u00edch v\u00e0 visualize Non-IID distribution c\u1ee7a data.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_client_distribution(client_data: List[Dict], num_classes: int):\n",
    "        \"\"\"\n",
    "        Ph\u00e2n t\u00edch label distribution c\u1ee7a m\u1ed7i client.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame v\u1edbi distribution stats\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"\ud83d\udcca NON-IID ANALYSIS: Client Data Distribution\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        stats = []\n",
    "        for cid, data in enumerate(client_data):\n",
    "            y = data['y_train'].cpu().numpy()\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            \n",
    "            # Create full distribution\n",
    "            full_dist = np.zeros(num_classes)\n",
    "            for u, c in zip(unique, counts):\n",
    "                full_dist[u] = c\n",
    "            \n",
    "            stats.append({\n",
    "                \"client_id\": cid,\n",
    "                \"total_samples\": len(y),\n",
    "                \"num_classes\": len(unique),\n",
    "                \"class_distribution\": full_dist,\n",
    "                \"dominant_class\": unique[np.argmax(counts)],\n",
    "                \"dominant_ratio\": counts.max() / len(y),\n",
    "            })\n",
    "            \n",
    "            print(f\"  Client {cid}: {len(y):,} samples, {len(unique)} classes, \"\n",
    "                  f\"dominant class {unique[np.argmax(counts)]} ({counts.max()/len(y)*100:.1f}%)\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_client_distribution(client_stats: List[Dict], num_classes: int, \n",
    "                                 save_path: str = None):\n",
    "        \"\"\"\n",
    "        V\u1ebd heatmap c\u1ee7a label distribution per client.\n",
    "        \"\"\"\n",
    "        # Build distribution matrix\n",
    "        num_clients = len(client_stats)\n",
    "        dist_matrix = np.zeros((num_clients, num_classes))\n",
    "        \n",
    "        for stat in client_stats:\n",
    "            cid = stat[\"client_id\"]\n",
    "            dist_matrix[cid] = stat[\"class_distribution\"]\n",
    "        \n",
    "        # Normalize to percentages\n",
    "        dist_matrix_pct = dist_matrix / dist_matrix.sum(axis=1, keepdims=True) * 100\n",
    "        \n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, max(6, num_clients * 0.5)))\n",
    "        \n",
    "        # Heatmap - raw counts\n",
    "        sns.heatmap(dist_matrix, ax=axes[0], cmap=\"YlOrRd\", \n",
    "                    xticklabels=[f\"C{i}\" for i in range(num_classes)],\n",
    "                    yticklabels=[f\"Client {i}\" for i in range(num_clients)],\n",
    "                    cbar_kws={\"label\": \"Sample Count\"})\n",
    "        axes[0].set_title(\"Label Distribution (Raw Counts)\", fontsize=12)\n",
    "        axes[0].set_xlabel(\"Class\")\n",
    "        axes[0].set_ylabel(\"Client\")\n",
    "        \n",
    "        # Heatmap - percentages\n",
    "        sns.heatmap(dist_matrix_pct, ax=axes[1], cmap=\"YlGnBu\", \n",
    "                    xticklabels=[f\"C{i}\" for i in range(num_classes)],\n",
    "                    yticklabels=[f\"Client {i}\" for i in range(num_clients)],\n",
    "                    cbar_kws={\"label\": \"Percentage (%)\"}, vmin=0, vmax=100)\n",
    "        axes[1].set_title(\"Label Distribution (Percentage)\", fontsize=12)\n",
    "        axes[1].set_xlabel(\"Class\")\n",
    "        axes[1].set_ylabel(\"Client\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"  \ud83d\udcbe Saved Non-IID plot: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: CNN-GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GRU_Model(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes: int = 34):\n",
    "        super().__init__()\n",
    "        if isinstance(input_shape, tuple):\n",
    "            seq_length = input_shape[0]\n",
    "        else:\n",
    "            seq_length = int(input_shape)\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # CNN blocks\n",
    "        self.conv1 = nn.Conv1d(1, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout_cnn1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout_cnn2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.dropout_cnn3 = nn.Dropout(0.3)\n",
    "\n",
    "        # Calculate CNN output size\n",
    "        cnn_len = seq_length\n",
    "        for _ in range(3):  # 3 pooling layers\n",
    "            cnn_len = cnn_len // 2\n",
    "        self.cnn_output_size = 256 * cnn_len\n",
    "\n",
    "        # GRU\n",
    "        self.gru1 = nn.GRU(1, 128, batch_first=True)\n",
    "        self.gru2 = nn.GRU(128, 64, batch_first=True)\n",
    "        self.gru_output_size = 64\n",
    "\n",
    "        # MLP\n",
    "        concat_size = self.cnn_output_size + self.gru_output_size\n",
    "        self.dense1 = nn.Linear(concat_size, 256)\n",
    "        self.bn_mlp1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dense2 = nn.Linear(256, 128)\n",
    "        self.bn_mlp2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.output = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # CNN\n",
    "        x_cnn = x.permute(0, 2, 1)\n",
    "        x_cnn = self.pool1(self.relu(self.bn1(self.conv1(x_cnn))))\n",
    "        x_cnn = self.dropout_cnn1(x_cnn)\n",
    "        x_cnn = self.pool2(self.relu(self.bn2(self.conv2(x_cnn))))\n",
    "        x_cnn = self.dropout_cnn2(x_cnn)\n",
    "        x_cnn = self.pool3(self.relu(self.bn3(self.conv3(x_cnn))))\n",
    "        x_cnn = self.dropout_cnn3(x_cnn)\n",
    "        cnn_output = x_cnn.view(batch_size, -1)\n",
    "\n",
    "        # GRU\n",
    "        x_gru = x\n",
    "        x_gru, _ = self.gru1(x_gru)\n",
    "        x_gru, _ = self.gru2(x_gru)\n",
    "        gru_output = x_gru[:, -1, :]\n",
    "\n",
    "        # Concat & MLP\n",
    "        z = torch.cat([cnn_output, gru_output], dim=1)\n",
    "        z = self.dense1(z)\n",
    "        if z.size(0) > 1:\n",
    "            z = self.bn_mlp1(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.dropout1(z)\n",
    "        z = self.dense2(z)\n",
    "        if z.size(0) > 1:\n",
    "            z = self.bn_mlp2(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.dropout2(z)\n",
    "        return self.output(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW: Pre-load Data into RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_client_data_v3(data_dir: str, num_clients: int, device: str):\n",
    "    \"\"\"\n",
    "    Load ALL client data into RAM at once (step2-version3 format).\n",
    "    \n",
    "    Returns:\n",
    "        client_data: List[Dict] with 'X_train', 'y_train' as tensors\n",
    "        test_data: Dict with 'X_test', 'y_test' as tensors\n",
    "        input_shape, num_classes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRE-LOADING ALL DATA INTO RAM (V3 Format)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    client_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Load each client train data\n",
    "    for cid in range(num_clients):\n",
    "        path = os.path.join(data_dir, f\"client_{cid}_train.npz\")\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing: {path}\")\n",
    "        \n",
    "        data = np.load(path)\n",
    "        X_train = data['X_train'].astype(np.float32)\n",
    "        y_train = data['y_train'].astype(np.int64)\n",
    "        \n",
    "        # Convert to tensors and move to device\n",
    "        X_train_t = torch.from_numpy(X_train).to(device)\n",
    "        y_train_t = torch.from_numpy(y_train).to(device)\n",
    "        \n",
    "        client_data.append({\n",
    "            'X_train': X_train_t,\n",
    "            'y_train': y_train_t,\n",
    "            'num_samples': len(y_train)\n",
    "        })\n",
    "        \n",
    "        all_labels.append(y_train)\n",
    "        \n",
    "        if (cid + 1) % 100 == 0 or cid == num_clients - 1:\n",
    "            print(f\"  Loaded client {cid+1}/{num_clients}: {len(y_train):,} samples\")\n",
    "    \n",
    "    # Load global test data\n",
    "    test_path = os.path.join(data_dir, \"global_test_data.npz\")\n",
    "    if not os.path.exists(test_path):\n",
    "        raise FileNotFoundError(f\"Missing: {test_path}\")\n",
    "    \n",
    "    test_npz = np.load(test_path)\n",
    "    X_test = test_npz['X_test'].astype(np.float32)\n",
    "    y_test = test_npz['y_test'].astype(np.int64)\n",
    "    \n",
    "    # Move to device\n",
    "    X_test_t = torch.from_numpy(X_test).to(device)\n",
    "    y_test_t = torch.from_numpy(y_test).to(device)\n",
    "    \n",
    "    test_data = {\n",
    "        'X_test': X_test_t,\n",
    "        'y_test': y_test_t,\n",
    "        'num_samples': len(y_test)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  \u2713 Loaded global test: {len(y_test):,} samples\")\n",
    "    \n",
    "    # Detect input_shape & num_classes\n",
    "    input_shape = (client_data[0]['X_train'].shape[1],)\n",
    "    all_labels_np = np.concatenate(all_labels)\n",
    "    num_classes = int(len(np.unique(all_labels_np)))\n",
    "    \n",
    "    total_train = sum(c['num_samples'] for c in client_data)\n",
    "    print(f\"\\n  input_shape: {input_shape}\")\n",
    "    print(f\"  num_classes: {num_classes}\")\n",
    "    print(f\"  total_train: {total_train:,}\")\n",
    "    print(f\"  total_test:  {len(y_test):,}\")\n",
    "    print(\"\\n  \u2713 All data pre-loaded into RAM!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return client_data, test_data, input_shape, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FederatedClient V2 (RAM-based)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedClientV2:\n",
    "    \"\"\"\n",
    "    Federated client that works with pre-loaded RAM data.\n",
    "    No DataLoader overhead - direct tensor operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, client_id: int, model: nn.Module, \n",
    "                 X_train: torch.Tensor, y_train: torch.Tensor, device: str):\n",
    "        self.client_id = client_id\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.device = device\n",
    "        self.num_samples = len(y_train)\n",
    "        \n",
    "        self.use_amp = (device == \"cuda\" and torch.cuda.is_available())\n",
    "    \n",
    "    def _amp_ctx(self):\n",
    "        return (\n",
    "            torch_autocast(device_type=\"cuda\", dtype=torch.float16)\n",
    "            if self.use_amp else contextlib.nullcontext()\n",
    "        )\n",
    "    \n",
    "    def _create_batches(self, batch_size: int):\n",
    "        \"\"\"Create random batches from pre-loaded data.\"\"\"\n",
    "        indices = torch.randperm(self.num_samples, device=self.device)\n",
    "        for i in range(0, self.num_samples, batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            yield self.X_train[batch_idx], self.y_train[batch_idx]\n",
    "    \n",
    "    def train_fedavg(self, epochs: int, batch_size: int, lr: float, verbose: bool = True):\n",
    "        self.model.train()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = GradScaler(enabled=self.use_amp)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for ep in range(epochs):\n",
    "            ep_loss = 0.0\n",
    "            ep_samples = 0\n",
    "            \n",
    "            for X_batch, y_batch in self._create_batches(batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with self._amp_ctx():\n",
    "                    out = self.model(X_batch)\n",
    "                    loss = criterion(out, y_batch)\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                bs = len(y_batch)\n",
    "                ep_loss += loss.item() * bs\n",
    "                ep_samples += bs\n",
    "            \n",
    "            total_loss += ep_loss\n",
    "            total_samples += ep_samples\n",
    "        \n",
    "        return {\n",
    "            \"client_id\": self.client_id,\n",
    "            \"num_samples\": self.num_samples,\n",
    "            \"loss\": total_loss / max(1, total_samples)\n",
    "        }\n",
    "    \n",
    "    def train_fedprox(self, epochs: int, batch_size: int, global_params: OrderedDict,\n",
    "                      mu: float, lr: float, verbose: bool = True):\n",
    "        self.model.train()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = GradScaler(enabled=self.use_amp)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for ep in range(epochs):\n",
    "            ep_loss = 0.0\n",
    "            ep_samples = 0\n",
    "            \n",
    "            for X_batch, y_batch in self._create_batches(batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with self._amp_ctx():\n",
    "                    out = self.model(X_batch)\n",
    "                    ce_loss = criterion(out, y_batch)\n",
    "                    \n",
    "                    # Proximal term\n",
    "                    prox = 0.0\n",
    "                    for name, param in self.model.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            gp = global_params[name].to(self.device)\n",
    "                            prox += torch.sum((param - gp)**2)\n",
    "                    \n",
    "                    loss = ce_loss + (mu / 2.0) * prox\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                bs = len(y_batch)\n",
    "                ep_loss += loss.item() * bs\n",
    "                ep_samples += bs\n",
    "            \n",
    "            total_loss += ep_loss\n",
    "            total_samples += ep_samples\n",
    "        \n",
    "        return {\n",
    "            \"client_id\": self.client_id,\n",
    "            \"num_samples\": self.num_samples,\n",
    "            \"loss\": total_loss / max(1, total_samples)\n",
    "        }\n",
    "    def train_fedplus(self, epochs: int, batch_size: int, global_params: OrderedDict,\n",
    "                      mu: float, lr: float, verbose: bool = True, use_sgd: bool = True):\n",
    "        \"\"\"\n",
    "        Train using Fed+ update rule (Eq. 8 in the paper).\n",
    "        \n",
    "        Paper: \"Federated Learning for Mobile Keyboard Prediction\" (Yang et al.)\n",
    "        Reference [290] in your citations.\n",
    "        \n",
    "        Update Rule (Eq. 8):\n",
    "            \u03b8 = 1 / (1 + \u03bd * \u03bb)       where \u03bd=learning_rate, \u03bb=mu\n",
    "            W_new = \u03b8 * W_updated + (1-\u03b8) * W_global\n",
    "        \n",
    "        Note: Paper derives this formula for vanilla SGD. Using Adam is an approximation\n",
    "        since Adam has adaptive per-parameter learning rates.\n",
    "        \n",
    "        Args:\n",
    "            use_sgd: If True (default), use SGD as per paper. If False, use Adam.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # SGD is recommended for Fed+ as the paper derives formula for fixed lr\n",
    "        if use_sgd:\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
    "        else:\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "            \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = GradScaler(enabled=self.use_amp)\n",
    "        \n",
    "        # Calculate theta: \u03b8 = 1 / (1 + \u03bd * \u03bb)\n",
    "        # \u03bd = learning rate, \u03bb = regularization strength (mu)\n",
    "        theta = 1.0 / (1.0 + lr * mu)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for ep in range(epochs):\n",
    "            ep_loss = 0.0\n",
    "            ep_samples = 0\n",
    "            \n",
    "            for X_batch, y_batch in self._create_batches(batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 1. Standard Forward & Backward\n",
    "                with self._amp_ctx():\n",
    "                    out = self.model(X_batch)\n",
    "                    loss = criterion(out, y_batch)\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()  # W = W - lr * grad\n",
    "                \n",
    "                # 2. Apply Fed+ Correction (Eq. 8)\n",
    "                # After optimizer step: W_local = W_old - lr * grad\n",
    "                # Now interpolate towards global: W = \u03b8 * W_local + (1-\u03b8) * W_global\n",
    "                with torch.no_grad():\n",
    "                    for name, param in self.model.named_parameters():\n",
    "                        if name in global_params:\n",
    "                            gp = global_params[name].to(self.device)\n",
    "                            param.data = theta * param.data + (1.0 - theta) * gp\n",
    "\n",
    "                bs = len(y_batch)\n",
    "                ep_loss += loss.item() * bs\n",
    "                ep_samples += bs\n",
    "            \n",
    "            total_loss += ep_loss\n",
    "            total_samples += ep_samples\n",
    "        \n",
    "        return {\n",
    "            \"client_id\": self.client_id,\n",
    "            \"num_samples\": self.num_samples,\n",
    "            \"loss\": total_loss / max(1, total_samples)\n",
    "        }\n",
    "    def get_model_params(self):\n",
    "        return OrderedDict((k, v.detach().cpu().clone())\n",
    "                           for k, v in self.model.state_dict().items())\n",
    "    \n",
    "    def set_model_params(self, params: OrderedDict):\n",
    "        self.model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FederatedServer V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedServerV2:\n",
    "    def __init__(self, global_model: nn.Module, clients: List[FederatedClientV2],\n",
    "                 test_data: Dict, device: str, config: Dict = None):\n",
    "        self.global_model = global_model.to(device)\n",
    "        self.clients = clients\n",
    "        self.test_data = test_data\n",
    "        self.device = device\n",
    "        self.num_classes = config.get(\"num_classes\", 34) if config else 34\n",
    "        \n",
    "        # Extended history v\u1edbi nhi\u1ec1u metrics h\u01a1n\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"test_loss\": [],\n",
    "            \"test_accuracy\": [],\n",
    "            \"test_f1_macro\": [],\n",
    "            \"test_f1_weighted\": [],\n",
    "            \"test_precision_macro\": [],\n",
    "            \"test_recall_macro\": [],\n",
    "            \"test_auc_macro\": [],\n",
    "        }\n",
    "    \n",
    "        # --- FedAvgM SETUP ---\n",
    "        self.server_momentum = config.get(\"server_momentum\", 0.0) if config else 0.0\n",
    "        self.server_lr = config.get(\"server_lr\", 1.0) if config else 1.0\n",
    "        \n",
    "        # Init velocity vectors for fedavgm\n",
    "        self.velocity = OrderedDict(\n",
    "            (k, torch.zeros_like(v).to(device)) \n",
    "            for k, v in global_model.state_dict().items()\n",
    "        )\n",
    "    \n",
    "    def get_global_params(self):\n",
    "        return OrderedDict((k, v.detach().clone())\n",
    "                           for k, v in self.global_model.state_dict().items())\n",
    "    \n",
    "    def set_global_params(self, params: OrderedDict):\n",
    "        self.global_model.load_state_dict(params)\n",
    "    \n",
    "    def distribute_model(self):\n",
    "        \"\"\"Distribute global model to ALL clients.\"\"\"\n",
    "        g = self.get_global_params()\n",
    "        for c in self.clients:\n",
    "            c.set_model_params(g)\n",
    "    \n",
    "    def aggregate_fedavg(self, results: List[Dict]):\n",
    "        total_samples = sum(r[\"num_samples\"] for r in results)\n",
    "        agg = self.get_global_params()\n",
    "        \n",
    "        for k, v in agg.items():\n",
    "            if v.dtype.is_floating_point:\n",
    "                agg[k] = torch.zeros_like(v)\n",
    "        \n",
    "        for r in results:\n",
    "            cid = r[\"client_id\"]\n",
    "            w_i = r[\"num_samples\"] / max(1, total_samples)\n",
    "            client_params = self.clients[cid].get_model_params()\n",
    "            \n",
    "            for k in agg.keys():\n",
    "                p = client_params[k].to(self.device)\n",
    "                if p.dtype.is_floating_point:\n",
    "                    agg[k] = agg[k] + w_i * p\n",
    "                else:\n",
    "                    agg[k] = p\n",
    "        return agg\n",
    "    \n",
    "    def train_round_fedavg(self, num_epochs: int, batch_size: int, lr: float,\n",
    "                           verbose: bool = True):\n",
    "        \"\"\"Train one round with FedAvg using ALL clients.\"\"\"\n",
    "        n_clients = len(self.clients)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\u2192 FedAvg: training ALL {n_clients} clients.\")\n",
    "        \n",
    "        self.distribute_model()\n",
    "        \n",
    "        results = []\n",
    "        for c in tqdm(self.clients, desc=\"Training clients\", leave=False):\n",
    "            r = c.train_fedavg(num_epochs, batch_size, lr, verbose=False)\n",
    "            results.append(r)\n",
    "        \n",
    "        new_params = self.aggregate_fedavg(results)\n",
    "        self.set_global_params(new_params)\n",
    "        \n",
    "        avg_loss = float(np.mean([r[\"loss\"] for r in results]))\n",
    "        if verbose:\n",
    "            print(f\"\u2192 Train loss: {avg_loss:.4f}\")\n",
    "        return {\"train_loss\": avg_loss}\n",
    "    \n",
    "    def train_round_fedprox(self, num_epochs: int, batch_size: int, mu: float, \n",
    "                            lr: float, verbose: bool = True):\n",
    "        \"\"\"Train one round with FedProx using ALL clients.\"\"\"\n",
    "        n_clients = len(self.clients)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\u2192 FedProx: training ALL {n_clients} clients.\")\n",
    "        \n",
    "        global_params = self.get_global_params()\n",
    "        self.distribute_model()\n",
    "        \n",
    "        results = []\n",
    "        for c in tqdm(self.clients, desc=\"Training clients\", leave=False):\n",
    "            r = c.train_fedprox(num_epochs, batch_size, global_params, mu, lr, verbose=False)\n",
    "            results.append(r)\n",
    "        \n",
    "        new_params = self.aggregate_fedavg(results)\n",
    "        self.set_global_params(new_params)\n",
    "        \n",
    "        avg_loss = float(np.mean([r[\"loss\"] for r in results]))\n",
    "        if verbose:\n",
    "            print(f\"\u2192 Train loss: {avg_loss:.4f}\")\n",
    "        return {\"train_loss\": avg_loss}\n",
    "    \n",
    "    def train_round_fedavgm(self, num_epochs: int, batch_size: int, lr: float, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Train one round with FedAvgM (Hsu et al., 2019).\n",
    "        \"\"\"\n",
    "        n_clients = len(self.clients)\n",
    "        if verbose:\n",
    "            print(f\"\u2192 FedAvgM (\u03b2={self.server_momentum}, \u03b7={self.server_lr}): training ALL {n_clients} clients.\")\n",
    "\n",
    "        w_t = self.get_global_params()\n",
    "        self.distribute_model()\n",
    "        \n",
    "        results = []\n",
    "        for c in tqdm(self.clients, desc=\"Training clients\", leave=False):\n",
    "            r = c.train_fedavg(num_epochs, batch_size, lr, verbose=False)\n",
    "            results.append(r)\n",
    "        \n",
    "        w_avg = self.aggregate_fedavg(results)\n",
    "        \n",
    "        beta = self.server_momentum\n",
    "        new_params = OrderedDict()\n",
    "        \n",
    "        for k in w_t.keys():\n",
    "            if w_t[k].dtype.is_floating_point:\n",
    "                curr_w = w_t[k].to(self.device)\n",
    "                avg_w = w_avg[k].to(self.device)\n",
    "                curr_v = self.velocity[k].to(self.device)\n",
    "                \n",
    "                delta = avg_w - curr_w\n",
    "                # FedAvgM momentum: v_{t+1} = \u03b2 * v_t + \u0394w (t\u00edch l\u0169y momentum)\n",
    "                new_v = beta * curr_v + delta\n",
    "                self.velocity[k] = new_v\n",
    "                # Update: w_{t+1} = w_t + \u03b7 * v_{t+1}\n",
    "                new_params[k] = curr_w + self.server_lr * new_v\n",
    "            else:\n",
    "                new_params[k] = w_avg[k]\n",
    "                \n",
    "        self.set_global_params(new_params)\n",
    "        \n",
    "        avg_loss = float(np.mean([r[\"loss\"] for r in results]))\n",
    "        if verbose:\n",
    "            print(f\"\u2192 Train loss: {avg_loss:.4f}\")\n",
    "        return {\"train_loss\": avg_loss}\n",
    "    \n",
    "    def train_round_fedplus(self, num_epochs: int, batch_size: int, mu: float, lr: float, verbose: bool = True):\n",
    "        \"\"\"Train one round with Fed+.\"\"\"\n",
    "        n_clients = len(self.clients)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\u2192 Fed+ (mu={mu}): training ALL {n_clients} clients.\")\n",
    "        \n",
    "        global_params = self.get_global_params()\n",
    "        self.distribute_model()\n",
    "        \n",
    "        results = []\n",
    "        for c in tqdm(self.clients, desc=\"Training clients\", leave=False):\n",
    "            r = c.train_fedplus(num_epochs, batch_size, global_params, mu, lr, verbose=False)\n",
    "            results.append(r)\n",
    "        \n",
    "        new_params = self.aggregate_fedavg(results)\n",
    "        self.set_global_params(new_params)\n",
    "        \n",
    "        avg_loss = float(np.mean([r[\"loss\"] for r in results]))\n",
    "        if verbose:\n",
    "            print(f\"\u2192 Train loss: {avg_loss:.4f}\")\n",
    "        return {\"train_loss\": avg_loss}\n",
    "    \n",
    "    def evaluate_global(self, batch_size: int = 1024, compute_auc: bool = True):\n",
    "        \"\"\"\n",
    "        Evaluate v\u1edbi \u0111\u1ea7y \u0111\u1ee7 metrics: Accuracy, F1, Precision, Recall, AUC.\n",
    "        \"\"\"\n",
    "        self.global_model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        X_test = self.test_data['X_test']\n",
    "        y_test = self.test_data['y_test']\n",
    "        n_test = len(y_test)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_proba = []\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_test, batch_size):\n",
    "                X_batch = X_test[i:i+batch_size]\n",
    "                y_batch = y_test[i:i+batch_size]\n",
    "                \n",
    "                out = self.global_model(X_batch)\n",
    "                loss = criterion(out, y_batch)\n",
    "                total_loss += loss.item() * len(y_batch)\n",
    "                \n",
    "                # Predictions\n",
    "                proba = F.softmax(out, dim=1)\n",
    "                preds = out.argmax(dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(y_batch.cpu().numpy())\n",
    "                all_proba.append(proba.cpu().numpy())\n",
    "        \n",
    "        # Convert to numpy\n",
    "        y_true = np.array(all_targets)\n",
    "        y_pred = np.array(all_preds)\n",
    "        y_proba = np.vstack(all_proba) if compute_auc else None\n",
    "        \n",
    "        # Compute all metrics\n",
    "        metrics = MetricsCalculator.compute_all_metrics(\n",
    "            y_true, y_pred, y_proba, self.num_classes\n",
    "        )\n",
    "        metrics[\"loss\"] = total_loss / n_test\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def evaluate_global_simple(self, batch_size: int = 1024):\n",
    "        \"\"\"Backward compatible - simple evaluation.\"\"\"\n",
    "        metrics = self.evaluate_global(batch_size, compute_auc=False)\n",
    "        return {\"accuracy\": metrics[\"accuracy\"], \"loss\": metrics[\"loss\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_federated_v3(server: FederatedServerV2, config: Dict, \n",
    "                       checkpoint_manager: CheckpointManager = None,\n",
    "                       start_round: int = 0):\n",
    "    \"\"\"\n",
    "    Training loop v\u1edbi checkpoint v\u00e0 extended metrics.\n",
    "    \n",
    "    Args:\n",
    "        server: FederatedServerV2\n",
    "        config: CONFIG dict\n",
    "        checkpoint_manager: CheckpointManager instance\n",
    "        start_round: Round to start from (for resume)\n",
    "    \"\"\"\n",
    "    algo = config[\"algorithm\"].lower()\n",
    "    R = config[\"num_rounds\"]\n",
    "    E = config[\"local_epochs\"]\n",
    "    bs = config[\"batch_size\"]\n",
    "    lr = config[\"learning_rate\"]\n",
    "    eval_every = config[\"eval_every\"]\n",
    "    save_every = config.get(\"save_checkpoint_every\", 1)\n",
    "    mu = config[\"mu\"]\n",
    "    \n",
    "    history = server.history\n",
    "    \n",
    "    for ridx in tqdm(range(start_round, R), desc=\"Global Rounds\", initial=start_round, total=R):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ROUND {ridx+1}/{R} ({algo})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Train\n",
    "        if algo == \"fedavg\":\n",
    "            r_res = server.train_round_fedavg(E, bs, lr, verbose=True)\n",
    "        elif algo == \"fedprox\":\n",
    "            r_res = server.train_round_fedprox(E, bs, mu, lr, verbose=True)\n",
    "        elif algo == \"fedavgm\":\n",
    "            r_res = server.train_round_fedavgm(E, bs, lr, verbose=True)\n",
    "        elif algo == \"fedplus\":\n",
    "            r_res = server.train_round_fedplus(E, bs, mu, lr, verbose=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algorithm: {algo}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        if (ridx + 1) % eval_every == 0:\n",
    "            print(\"\\n  \ud83d\udcca Evaluating global model...\")\n",
    "            metrics = server.evaluate_global(bs, compute_auc=True)\n",
    "            \n",
    "            # Update history\n",
    "            history[\"train_loss\"].append(r_res[\"train_loss\"])\n",
    "            history[\"test_loss\"].append(metrics[\"loss\"])\n",
    "            history[\"test_accuracy\"].append(metrics[\"accuracy\"])\n",
    "            history[\"test_f1_macro\"].append(metrics[\"f1_macro\"])\n",
    "            history[\"test_f1_weighted\"].append(metrics[\"f1_weighted\"])\n",
    "            history[\"test_precision_macro\"].append(metrics[\"precision_macro\"])\n",
    "            history[\"test_recall_macro\"].append(metrics[\"recall_macro\"])\n",
    "            history[\"test_auc_macro\"].append(metrics.get(\"auc_macro_ovr\"))\n",
    "            \n",
    "            # Print summary\n",
    "            MetricsCalculator.print_metrics_summary(metrics, ridx)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if checkpoint_manager and (ridx + 1) % save_every == 0:\n",
    "            checkpoint_manager.save_checkpoint(ridx, server, history, config)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Pre-load all data into RAM\n",
    "client_data, test_data, input_shape, num_classes = load_all_client_data_v3(\n",
    "    CONFIG[\"data_dir\"],\n",
    "    CONFIG[\"num_clients\"],\n",
    "    CONFIG[\"device\"]\n",
    ")\n",
    "\n",
    "CONFIG[\"input_shape\"] = input_shape\n",
    "CONFIG[\"num_classes\"] = num_classes\n",
    "\n",
    "print(f\"\\nConfig: {json.dumps(CONFIG, indent=2, default=str)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize models & clients\n",
    "print(\"\\nInitializing global model & clients...\")\n",
    "\n",
    "global_model = CNN_GRU_Model(input_shape, num_classes).to(device)\n",
    "base_state = global_model.state_dict()\n",
    "\n",
    "clients = []\n",
    "for cid in range(CONFIG[\"num_clients\"]):\n",
    "    m = CNN_GRU_Model(input_shape, num_classes).to(device)\n",
    "    m.load_state_dict(base_state)\n",
    "    \n",
    "    c = FederatedClientV2(\n",
    "        client_id=cid,\n",
    "        model=m,\n",
    "        X_train=client_data[cid]['X_train'],\n",
    "        y_train=client_data[cid]['y_train'],\n",
    "        device=device\n",
    "    )\n",
    "    clients.append(c)\n",
    "\n",
    "server = FederatedServerV2(global_model, clients, test_data, device, config=CONFIG)\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "ckpt_manager = CheckpointManager(CONFIG[\"checkpoint_dir\"], CONFIG[\"algorithm\"], CONFIG[\"num_clients\"])\n",
    "\n",
    "print(f\"\u2713 Initialized {len(clients)} clients\")\n",
    "\n",
    "# Analyze Non-IID distribution\n",
    "noniid_stats = NonIIDAnalyzer.analyze_client_distribution(client_data, num_classes)\n",
    "# Visualization \u0111\u00e3 chuy\u1ec3n sang script non_iid_visualize.py \u0111\u1ec3 tr\u00e1nh l\u1ed7i k\u00edch th\u01b0\u1edbc khi s\u1ed1 client l\u1edbn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train with checkpoints\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Check for resume\n",
    "start_round = 0\n",
    "if CONFIG.get(\"resume_from_checkpoint\"):\n",
    "    start_round, _ = CheckpointManager.load_checkpoint(\n",
    "        CONFIG[\"resume_from_checkpoint\"], server, device\n",
    "    )\n",
    "    start_round += 1  # Start from next round\n",
    "\n",
    "history = train_federated_v3(server, CONFIG, ckpt_manager, start_round)\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n\u2713 Training complete! Duration: {duration:.2f}s ({duration/60:.2f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(server, num_classes: int, class_names: List[str] = None,\n",
    "                    save_path: str = None, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    V\u1ebd ROC curves cho multiclass classification (\u0110\u00e3 fix l\u1ed7i NaN).\n",
    "    \"\"\"\n",
    "    print(\"\\n\ud83d\udcc8 Generating ROC Curves...\")\n",
    "    server.global_model.eval()\n",
    "    \n",
    "    X_test = server.test_data['X_test']\n",
    "    y_test = server.test_data['y_test']\n",
    "    batch_size = 4096\n",
    "    \n",
    "    all_proba = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(y_test), batch_size), desc=\"Computing probabilities\"):\n",
    "            X_batch = X_test[i:i+batch_size]\n",
    "            y_batch = y_test[i:i+batch_size]\n",
    "            \n",
    "            out = server.global_model(X_batch)\n",
    "            proba = F.softmax(out, dim=1)\n",
    "            \n",
    "            all_proba.append(proba.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(all_targets)\n",
    "    y_proba = np.vstack(all_proba)\n",
    "    \n",
    "    # --- PH\u1ea6N S\u1eecA L\u1ed6I (FIX NaN) ---\n",
    "    # Ki\u1ec3m tra xem c\u00f3 gi\u00e1 tr\u1ecb NaN hay Infinity kh\u00f4ng\n",
    "    if not np.isfinite(y_proba).all():\n",
    "        print(\"\\n\u26a0\ufe0f WARNING: Ph\u00e1t hi\u1ec7n NaN/Inf trong d\u1ef1 \u0111o\u00e1n c\u1ee7a model!\")\n",
    "        print(\"  -> \u0110ang thay th\u1ebf NaN b\u1eb1ng 0.0 \u0111\u1ec3 ti\u1ebfp t\u1ee5c v\u1ebd bi\u1ec3u \u0111\u1ed3.\")\n",
    "        # Thay th\u1ebf NaN b\u1eb1ng 0.0, Inf b\u1eb1ng s\u1ed1 c\u1ef1c l\u1edbn/nh\u1ecf gi\u1edbi h\u1ea1n\n",
    "        y_proba = np.nan_to_num(y_proba, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    # ------------------------------\n",
    "    \n",
    "    # Binarize labels\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "    \n",
    "    # Handle case where not all classes are present in test set\n",
    "    if y_true_bin.shape[1] != num_classes:\n",
    "        # Re-create with correct shape if needed\n",
    "        temp = np.zeros((len(y_true), num_classes))\n",
    "        temp[:, :y_true_bin.shape[1]] = y_true_bin\n",
    "        y_true_bin = temp\n",
    "\n",
    "    # Compute ROC curve and AUC for each class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        # Ch\u1ec9 t\u00ednh n\u1ebfu class \u0111\u00f3 c\u00f3 \u00edt nh\u1ea5t 1 sample positive\n",
    "        if y_true_bin[:, i].sum() > 0:\n",
    "            try:\n",
    "                fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            except Exception as e:\n",
    "                print(f\"  \u26a0 L\u1ed7i t\u00ednh ROC class {i}: {e}\")\n",
    "                roc_auc[i] = None\n",
    "        else:\n",
    "            roc_auc[i] = None\n",
    "    \n",
    "    # Sort by AUC to get top-k\n",
    "    valid_aucs = [(i, val) for i, val in roc_auc.items() if val is not None]\n",
    "    # S\u1eafp x\u1ebfp gi\u1ea3m d\u1ea7n, x\u1eed l\u00fd tr\u01b0\u1eddng h\u1ee3p val c\u00f3 th\u1ec3 b\u1ecb NaN d\u00f9 \u0111\u00e3 nan_to_num (\u00edt g\u1eb7p)\n",
    "    valid_aucs.sort(key=lambda x: x[1] if not np.isnan(x[1]) else -1, reverse=True)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: All ROC curves (Limit to top 20 to avoid clutter)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, min(20, num_classes)))\n",
    "    plot_limit = min(20, len(valid_aucs))\n",
    "    \n",
    "    for idx, (i, auc_val) in enumerate(valid_aucs[:plot_limit]): \n",
    "        label = class_names[i] if class_names else f\"Class {i}\"\n",
    "        val_str = f\"{auc_val:.3f}\" if auc_val is not None else \"NaN\"\n",
    "        axes[0].plot(fpr[i], tpr[i], color=colors[idx % 20], lw=1, alpha=0.7,\n",
    "                    label=f'{label} (AUC={val_str})')\n",
    "    \n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', lw=1, label='Random')\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title(f'ROC Curves - Top {plot_limit} Classes')\n",
    "    axes[0].legend(loc='lower right', fontsize=7)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Top-k classes specific\n",
    "    actual_top_k = min(top_k, len(valid_aucs))\n",
    "    for idx, (i, auc_val) in enumerate(valid_aucs[:actual_top_k]):\n",
    "        label = class_names[i] if class_names else f\"Class {i}\"\n",
    "        val_str = f\"{auc_val:.3f}\" if auc_val is not None else \"NaN\"\n",
    "        axes[1].plot(fpr[i], tpr[i], lw=1, label=f'{label} (AUC={val_str})')\n",
    "    \n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', lw=1, label='Random')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title(f'ROC Curves - Top {actual_top_k} Classes by AUC')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  \ud83d\udcbe Saved ROC curves: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print AUC summary\n",
    "    print(f\"\\n\ud83d\udcca AUC Summary (Top {actual_top_k}):\")\n",
    "    for i, (cls_idx, auc_val) in enumerate(valid_aucs[:actual_top_k]):\n",
    "        label = class_names[cls_idx] if class_names else f\"Class {cls_idx}\"\n",
    "        val_str = f\"{auc_val:.4f}\" if auc_val is not None else \"NaN\"\n",
    "        print(f\"  {i+1}. {label}: {val_str}\")\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history: Dict, save_path: str = None):\n",
    "    \"\"\"\n",
    "    V\u1ebd bi\u1ec3u \u0111\u1ed3 training curves v\u1edbi t\u1ea5t c\u1ea3 metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    rounds = range(len(history[\"test_loss\"]))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axes[0, 0].plot(rounds, history[\"test_loss\"], 'b-o', label=\"Test Loss\")\n",
    "    axes[0, 0].plot(rounds, history[\"train_loss\"], 'r-s', label=\"Train Loss\")\n",
    "    axes[0, 0].set_xlabel(\"Round\")\n",
    "    axes[0, 0].set_ylabel(\"Loss\")\n",
    "    axes[0, 0].set_title(\"Loss\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy\n",
    "    acc = [a * 100 for a in history[\"test_accuracy\"]]\n",
    "    axes[0, 1].plot(rounds, acc, 'g-o', linewidth=1)\n",
    "    axes[0, 1].set_xlabel(\"Round\")\n",
    "    axes[0, 1].set_ylabel(\"Accuracy (%)\")\n",
    "    axes[0, 1].set_title(\"Test Accuracy\")\n",
    "    axes[0, 1].set_ylim([0, 100])\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: F1 Score\n",
    "    f1_macro = [f * 100 for f in history[\"test_f1_macro\"]]\n",
    "    f1_weighted = [f * 100 for f in history[\"test_f1_weighted\"]]\n",
    "    axes[0, 2].plot(rounds, f1_macro, 'purple', marker='o', label=\"F1 Macro\")\n",
    "    axes[0, 2].plot(rounds, f1_weighted, 'orange', marker='s', label=\"F1 Weighted\")\n",
    "    axes[0, 2].set_xlabel(\"Round\")\n",
    "    axes[0, 2].set_ylabel(\"F1 Score (%)\")\n",
    "    axes[0, 2].set_title(\"F1 Score\")\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Precision & Recall\n",
    "    prec = [p * 100 for p in history[\"test_precision_macro\"]]\n",
    "    rec = [r * 100 for r in history[\"test_recall_macro\"]]\n",
    "    axes[1, 0].plot(rounds, prec, 'b-o', label=\"Precision (Macro)\")\n",
    "    axes[1, 0].plot(rounds, rec, 'r-s', label=\"Recall (Macro)\")\n",
    "    axes[1, 0].set_xlabel(\"Round\")\n",
    "    axes[1, 0].set_ylabel(\"Score (%)\")\n",
    "    axes[1, 0].set_title(\"Precision & Recall\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: AUC\n",
    "    auc_vals = history.get(\"test_auc_macro\", [])\n",
    "    if auc_vals and auc_vals[0] is not None:\n",
    "        auc_pct = [a * 100 if a else 0 for a in auc_vals]\n",
    "        axes[1, 1].plot(rounds, auc_pct, 'teal', marker='o', linewidth=1)\n",
    "        axes[1, 1].set_xlabel(\"Round\")\n",
    "        axes[1, 1].set_ylabel(\"AUC (%)\")\n",
    "        axes[1, 1].set_title(\"AUC (Macro OvR) \")\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, \"AUC not computed\", ha='center', va='center')\n",
    "        axes[1, 1].set_title(\"AUC (Macro OvR)\")\n",
    "    \n",
    "    # Plot 6: All metrics comparison (final values)\n",
    "    final_metrics = {\n",
    "        \"Accuracy\": history[\"test_accuracy\"][-1] * 100,\n",
    "        \"F1 Macro\": history[\"test_f1_macro\"][-1] * 100,\n",
    "        \"F1 Weighted\": history[\"test_f1_weighted\"][-1] * 100,\n",
    "        \"Precision\": history[\"test_precision_macro\"][-1] * 100,\n",
    "        \"Recall\": history[\"test_recall_macro\"][-1] * 100,\n",
    "    }\n",
    "    if auc_vals and auc_vals[-1] is not None:\n",
    "        final_metrics[\"AUC\"] = auc_vals[-1] * 100\n",
    "    \n",
    "    bars = axes[1, 2].bar(final_metrics.keys(), final_metrics.values(), \n",
    "                          color=['green', 'purple', 'orange', 'blue', 'red', 'teal'][:len(final_metrics)])\n",
    "    axes[1, 2].set_ylabel(\"Score (%)\")\n",
    "    axes[1, 2].set_title(\"Final Metrics Summary\")\n",
    "    axes[1, 2].set_ylim([0, 100])\n",
    "    for bar, val in zip(bars, final_metrics.values()):\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                       f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"\ud83d\udcbe Saved training curves: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model & history\n",
    "# D\u00f9ng c\u00f9ng folder name v\u1edbi checkpoint (algorithm_num_clients)\n",
    "folder_name = ckpt_manager.get_folder_name()\n",
    "out_dir = os.path.join(CONFIG[\"output_dir\"], folder_name)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "algo_name = CONFIG[\"algorithm\"]\n",
    "\n",
    "# Save model v\u1edbi t\u00ean t\u1ef1 \u0111\u1ed9ng: <algo>_global_model_<time>.pth\n",
    "model_filename = make_auto_filename(\n",
    "    prefix=f\"{algo_name}_global_model\",\n",
    "    ext=\"pth\",\n",
    ")\n",
    "model_path = os.path.join(out_dir, model_filename)\n",
    "torch.save(server.global_model.state_dict(), model_path)\n",
    "print(f\"Saved model: {model_path}\")\n",
    "\n",
    "# Save full history v\u1edbi t\u00ean t\u1ef1 \u0111\u1ed9ng: <algo>_global_history_<time>.json\n",
    "hist_filename = make_auto_filename(\n",
    "    prefix=f\"{algo_name}_global_history\",\n",
    "    ext=\"json\",\n",
    ")\n",
    "hist_path = os.path.join(out_dir, hist_filename)\n",
    "with open(hist_path, \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"Saved history: {hist_path}\")\n",
    "\n",
    "# Plot all training curves: <algo>_training_curves_<time>.png\n",
    "curves_filename = make_auto_filename(\n",
    "    prefix=f\"{algo_name}_training_curves\",\n",
    "    ext=\"png\",\n",
    ")\n",
    "plot_training_curves(history, save_path=os.path.join(out_dir, curves_filename))\n",
    "\n",
    "# Plot ROC curves: <algo>_roc_curves_<time>.png\n",
    "roc_filename = make_auto_filename(\n",
    "    prefix=f\"{algo_name}_roc_curves\",\n",
    "    ext=\"png\",\n",
    ")\n",
    "plot_roc_curves(server, num_classes, save_path=os.path.join(out_dir, roc_filename))\n",
    "\n",
    "# Save run_info.json v\u1edbi metadata t\u1ed5ng h\u1ee3p\n",
    "def save_run_info(out_dir, config, history, start_time, end_time, \n",
    "                  model_filename, hist_filename, curves_filename, roc_filename, cm_filename=None):\n",
    "    \"\"\"L\u01b0u th\u00f4ng tin t\u1ed5ng h\u1ee3p c\u1ee7a l\u1ea7n train\"\"\"\n",
    "    # T\u00ednh best metrics\n",
    "    best_auc = None\n",
    "    if history.get(\"test_auc_macro\"):\n",
    "        auc_values = [a for a in history[\"test_auc_macro\"] if a is not None]\n",
    "        if auc_values:\n",
    "            best_auc = max(auc_values) * 100\n",
    "    \n",
    "    run_info = {\n",
    "        \"run_id\": ckpt_manager.get_run_id(),\n",
    "        \"algorithm\": config[\"algorithm\"],\n",
    "        \"timestamp\": {\n",
    "            \"start\": start_time.isoformat(),\n",
    "            \"end\": end_time.isoformat(),\n",
    "            \"duration_seconds\": (end_time - start_time).total_seconds(),\n",
    "            \"duration_minutes\": (end_time - start_time).total_seconds() / 60\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"num_clients\": config[\"num_clients\"],\n",
    "            \"num_rounds\": config[\"num_rounds\"],\n",
    "            \"local_epochs\": config[\"local_epochs\"],\n",
    "            \"learning_rate\": config[\"learning_rate\"],\n",
    "            \"batch_size\": config[\"batch_size\"],\n",
    "            \"mu\": config.get(\"mu\"),\n",
    "            \"server_momentum\": config.get(\"server_momentum\"),\n",
    "            \"server_lr\": config.get(\"server_lr\"),\n",
    "        },\n",
    "        \"best_metrics\": {\n",
    "            \"best_accuracy\": max(history[\"test_accuracy\"]) * 100,\n",
    "            \"best_f1_macro\": max(history[\"test_f1_macro\"]) * 100,\n",
    "            \"best_f1_weighted\": max(history[\"test_f1_weighted\"]) * 100,\n",
    "            \"best_auc\": best_auc,\n",
    "            \"final_accuracy\": history[\"test_accuracy\"][-1] * 100,\n",
    "            \"final_f1_macro\": history[\"test_f1_macro\"][-1] * 100,\n",
    "        },\n",
    "        \"files\": {\n",
    "            \"model\": model_filename,\n",
    "            \"history\": hist_filename,\n",
    "            \"training_curves\": curves_filename,\n",
    "            \"roc_curves\": roc_filename,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if cm_filename:\n",
    "        run_info[\"files\"][\"confusion_matrix\"] = cm_filename\n",
    "    \n",
    "    info_path = os.path.join(out_dir, \"run_info.json\")\n",
    "    with open(info_path, \"w\") as f:\n",
    "        json.dump(run_info, f, indent=2)\n",
    "    print(f\"\ud83d\udcbe Saved run info: {info_path}\")\n",
    "    return run_info\n",
    "\n",
    "# L\u01b0u run_info (cm_filename s\u1ebd \u0111\u01b0\u1ee3c th\u00eam sau khi v\u1ebd confusion matrix)\n",
    "run_info = save_run_info(out_dir, CONFIG, history, start_time, end_time,\n",
    "                         model_filename, hist_filename, curves_filename, roc_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def generate_confusion_matrix(server, class_names=None, out_dir=None):\n",
    "    \"\"\"\n",
    "    D\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test to\u00e0n c\u1ee5c v\u00e0 v\u1ebd Confusion Matrix.\n",
    "    \n",
    "    Args:\n",
    "        server: FederatedServerV2 instance\n",
    "        class_names: List t\u00ean c\u00e1c class (optional)\n",
    "        out_dir: Th\u01b0 m\u1ee5c \u0111\u1ec3 l\u01b0u file (n\u1ebfu None th\u00ec d\u00f9ng CONFIG[\"output_dir\"])\n",
    "    \n",
    "    Returns:\n",
    "        cm_filename: T\u00ean file \u0111\u00e3 l\u01b0u\n",
    "    \"\"\"\n",
    "    print(\"\\nGenering Confusion Matrix...\")\n",
    "    server.global_model.eval()\n",
    "    \n",
    "    X_test = server.test_data['X_test']\n",
    "    y_test = server.test_data['y_test']\n",
    "    batch_size = 4096 # D\u00f9ng batch l\u1edbn \u0111\u1ec3 infer cho nhanh\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(y_test), batch_size), desc=\"Inferencing\"):\n",
    "            X_batch = X_test[i:i+batch_size]\n",
    "            y_batch = y_test[i:i+batch_size]\n",
    "            \n",
    "            outputs = server.global_model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "            \n",
    "    # T\u00ednh to\u00e1n Confusion Matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Chu\u1ea9n b\u1ecb v\u1ebd\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # N\u1ebfu kh\u00f4ng c\u00f3 t\u00ean class c\u1ee5 th\u1ec3, t\u1ea1o t\u00ean d\u1ea1ng Class 0, Class 1...\n",
    "    if class_names is None:\n",
    "        num_classes = cm.shape[0]\n",
    "        class_names = [f\"Class {i}\" for i in range(num_classes)]\n",
    "    \n",
    "    # V\u1ebd Heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    plt.title(f'Confusion Matrix ( Rounds: {len(server.history[\"test_loss\"])})', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # L\u01b0u \u1ea3nh v\u1edbi t\u00ean t\u1ef1 \u0111\u1ed9ng: <algo>_confusion_matrix_<time>.png\n",
    "    algo_name = CONFIG[\"algorithm\"]\n",
    "    cm_filename = make_auto_filename(\n",
    "        prefix=f\"{algo_name}_confusion_matrix\",\n",
    "        ext=\"png\",\n",
    "    )\n",
    "    # L\u01b0u v\u00e0o c\u00f9ng folder v\u1edbi k\u1ebft qu\u1ea3 kh\u00e1c\n",
    "    if out_dir is None:\n",
    "        out_dir = CONFIG[\"output_dir\"]\n",
    "    save_path = os.path.join(out_dir, cm_filename)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved Confusion Matrix to: {save_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return cm_filename\n",
    "\n",
    "# --- CH\u1ea0Y H\u00c0M V\u1ebc ---\n",
    "# N\u1ebfu b\u1ea1n c\u00f3 danh s\u00e1ch t\u00ean c\u00e1c lo\u1ea1i t\u1ea5n c\u00f4ng (34 lo\u1ea1i), h\u00e3y \u0111i\u1ec1n v\u00e0o \u0111\u00e2y.\n",
    "class_names_list = [\n",
    "    \"Benign\",\n",
    "\n",
    "    \"ACK fragmentation\",\n",
    "    \"UDP flood\",\n",
    "    \"SlowLoris\",\n",
    "    \"ICMP flood\",\n",
    "    \"RSTFIN flood\",\n",
    "    \"PSHACK flood\",\n",
    "    \"HTTP flood\",\n",
    "    \"UDP fragmentation\",\n",
    "    \"TCP flood\",\n",
    "    \"SYN flood\",\n",
    "    \"SynonymousIP flood\",\n",
    "\n",
    "    \n",
    "    \"Dictionary brute force\",\n",
    "\n",
    "    \n",
    "    \"Arp spoofing\",\n",
    "    \"DNS spoofing\",\n",
    "\n",
    "    \n",
    "    \"TCP flood (DoS)\",\n",
    "    \"HTTP flood (DoS)\",\n",
    "    \"SYN flood (DoS)\",\n",
    "    \"UDP flood (DoS)\",\n",
    "\n",
    "    \n",
    "    \"Ping sweep\",\n",
    "    \"OS scan\",\n",
    "    \"Vulnerability scan\",\n",
    "    \"Port scan\",\n",
    "    \"Host discovery\",\n",
    "\n",
    "    \n",
    "    \"Sql injection\",\n",
    "    \"Command injection\",\n",
    "    \"Backdoor malware\",\n",
    "    \"Uploading attack\",\n",
    "    \"XSS\",\n",
    "    \"Browser hijacking\",\n",
    "\n",
    "    \n",
    "    \"GREIP flood\",\n",
    "    \"Greeth flood\",\n",
    "    \"UDPPlain\"\n",
    "]\n",
    "\n",
    "# N\u1ebfu kh\u00f4ng, \u0111\u1ec3 None n\u00f3 s\u1ebd t\u1ef1 \u0111\u00e1nh s\u1ed1 0-33.\n",
    "\n",
    "# V\u1ebd v\u00e0 l\u01b0u Confusion Matrix v\u00e0o c\u00f9ng folder v\u1edbi k\u1ebft qu\u1ea3 kh\u00e1c\n",
    "cm_filename = generate_confusion_matrix(server, class_names=class_names_list, out_dir=out_dir)\n",
    "\n",
    "# C\u1eadp nh\u1eadt run_info.json v\u1edbi cm_filename\n",
    "run_info[\"files\"][\"confusion_matrix\"] = cm_filename\n",
    "info_path = os.path.join(out_dir, \"run_info.json\")\n",
    "with open(info_path, \"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "print(f\"\ud83d\udcbe Updated run_info.json with confusion matrix filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Features of FedLearning V2:\n",
    "\n",
    "1. **\u2705 Compatible with step2-version3 format**\n",
    "\n",
    "   - Loads `client_X_train.npz` (train only)\n",
    "   - Loads `global_test_data.npz` (independent test)\n",
    "\n",
    "2. **\u2705 Pre-loads all data into RAM**\n",
    "\n",
    "   - Load once at startup\n",
    "   - Fast training (no disk I/O per epoch)\n",
    "   - Direct tensor operations\n",
    "\n",
    "3. **\u2705 Optimized for large-scale FL**\n",
    "   - Supports 1000+ clients\n",
    "   - **Always uses ALL clients** (no client fraction sampling)\n",
    "   - AMP support\n",
    "\n",
    "### Performance Comparison:\n",
    "\n",
    "| Metric          | V1 (DataLoader) | V2 (RAM)            |\n",
    "| --------------- | --------------- | ------------------- |\n",
    "| Data loading    | Per epoch       | Once                |\n",
    "| Disk I/O        | High            | Low                 |\n",
    "| Training speed  | ~1x             | ~2-3x faster        |\n",
    "| RAM usage       | Low             | High                |\n",
    "| Client sampling | Yes             | **No (always all)** |\n",
    "\n",
    "**Best for**: Large RAM machines, repeated training runs, scenarios where all clients participate in every round\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8766973,
     "sourceId": 13774402,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}